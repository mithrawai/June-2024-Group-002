{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/women-in-ai-ireland/June-2024-Group-002/blob/main/Test3_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes:\n",
        "\n",
        "- Replace model to google/gemma-2b as it's smaller\n",
        "- Set AutoTokenizer max_length=256\n",
        "- Remove printing document chunks (just in case is using up memory)\n",
        "- Ingest and Chunks function: Set path to \"{root_dir}/data\" (I think that's your path?)\n",
        "- Reduce text splitter to 500 and overlapping of 50 (just checking if this helps with the memory issue)\n",
        "- Reduce prompt to 100\n",
        "- Retrieve relevants chunks funtion: reduce to 3 embeddings"
      ],
      "metadata": {
        "id": "zl2wDL8uWTPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeUcHZZkwcgI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install pytorch torchvision torchaudio\n",
        "!pip install transformers==4.30\n",
        "!pip install langchain sentence_transformers huggingface-hub\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-community\n",
        "!pip install bitsandbytes\n",
        "!pip install faiss-cpu langchain-openai tiktoken unstructured selenium newspaper3k textstat\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "import os\n",
        "import pickle\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "XbMEhBa4w5A4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/MyDrive/WAI_project/\""
      ],
      "metadata": {
        "id": "AXpd4f-3w5Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents\n",
        "loader = DirectoryLoader(f\"{root_dir}\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "YMO_Un5_w0Lp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_y6TMi_wdHi",
        "outputId": "756cabf4-c68a-4351-ccc8-2716995b99e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set HF token\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ[\"HF_TOKEN\"] = hf_token"
      ],
      "metadata": {
        "id": "v3TCpbThwdKW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer with quantization\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config, model_max_length=256)\n",
        "\n"
      ],
      "metadata": {
        "id": "KwlBTwzCwdM8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        ")\n",
        "def ingest_and_chunk_pdfs():\n",
        "    loader = DirectoryLoader(f\"{root_dir}/data\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Debug: Print the number of documents loaded\n",
        "    '''print(f\"Number of documents loaded: {len(documents)}\")\n",
        "    for i, doc in enumerate(documents[:3]):\n",
        "        print(f\"Document {i+1}:\")\n",
        "        print(doc.page_content[:500])'''\n",
        "\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Debug: Print the number of text chunks created\n",
        "    '''print(f\"Number of text chunks: {len(texts)}\")\n",
        "    for i, text in enumerate(texts[:3]):\n",
        "        print(f\"Chunk {i+1}:\")\n",
        "        print(text.page_content[:500])'''\n",
        "    return texts\n",
        "\n",
        "texts = ingest_and_chunk_pdfs()"
      ],
      "metadata": {
        "id": "DkYXl5prwdPy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "0RXSvk2lwdSk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_embeddings(docs, store_name, path):\n",
        "    embeddings = generate_embeddings([doc.page_content for doc in docs])\n",
        "    vector_store = FAISS.from_documents(docs, embeddings)\n",
        "    with open(os.path.join(path, f\"faiss_{store_name}.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(vector_store, f)\n",
        "\n",
        "def load_embeddings(store_name, path):\n",
        "    with open(os.path.join(path, f\"faiss_{store_name}.pkl\"), \"rb\") as f:\n",
        "        vector_store = pickle.load(f)\n",
        "    return vector_store\n"
      ],
      "metadata": {
        "id": "jDDjUBc0wdVK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks(question, vector_store, num_chunks=3):\n",
        "    docs = vector_store.similarity_search(question, k=num_chunks)\n",
        "    return docs"
      ],
      "metadata": {
        "id": "BvOmBwOpwdYE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(question, chunks):\n",
        "    context = \"\\n\".join([chunk.page_content for chunk in chunks])\n",
        "    prompt = f\"Provide an answer to the following question using only the context provided: {question}? \" \\\n",
        "             f\"If you cannot answer this question from the information provided, respond with 'There is insufficient information to answer this question.'\\n\\n{context}\"\n",
        "    return prompt\n",
        "\n",
        "def gen_answer(prompt, max_length=100, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    response = model.generate(inputs, max_new_tokens=max_length, temperature=temperature)\n",
        "    answer = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    return answer.strip()"
      ],
      "metadata": {
        "id": "dvkThl9Xwdap"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(question):\n",
        "    # Ensure embeddings are stored\n",
        "    store_embeddings(texts, \"embedding_store\", root_dir)\n",
        "\n",
        "    # Load the vector store\n",
        "    vector_store = load_embeddings(\"embedding_store\", root_dir)\n",
        "\n",
        "    # Retrieve relevant chunks based on the question\n",
        "    relevant_chunks = retrieve_relevant_chunks(question, vector_store)\n",
        "\n",
        "    # Format the prompt for the LLM\n",
        "    prompt = format_prompt(question, relevant_chunks)\n",
        "\n",
        "    # Generate the answer using the LLM\n",
        "    answer = gen_answer(prompt)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "1rXsO949wdf6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"What are the main causes of climate change?\"\n",
        "answer = main(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "Ia_wMX8fwdjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}