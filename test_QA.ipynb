{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPtvC9ecXPhLNq/1KU+WEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/women-in-ai-ireland/June-2024-Group-002/blob/main/test_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install pytorch torchvision torchaudio\n",
        "!pip install transformers==4.30\n",
        "!pip install langchain sentence_transformers huggingface-hub\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-community\n",
        "!pip install bitsandbytes\n",
        "!pip install faiss-cpu langchain-openai tiktoken unstructured selenium newspaper3k textstat\n",
        "!pip install accelerate\n",
        "\n",
        "!pip install langchain-huggingface\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install InstructorEmbedding"
      ],
      "metadata": {
        "id": "Yup-yE3u4WOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "import os\n",
        "import pickle\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "metadata": {
        "id": "zl4J2X1idmH_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/MyDrive/WAI_project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrCNobZ5dppM",
        "outputId": "48298e62-cbf2-42cf-dce6-6a97e0b387e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "wCVu7yok1xSH",
        "outputId": "5ef474fb-2f46-4502-aa88-9f515d4a8601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set HF token\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "-eUsWUXId1Gs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load instructor embeddings model\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})"
      ],
      "metadata": {
        "id": "7TFACEiV4SCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set local path to store embeddings ***replace with SingleStore, AWS or similar\n",
        "embedding_store_path = f\"{root_dir}/embedding_store\""
      ],
      "metadata": {
        "id": "P3-AvwRgd6xX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defines the parameters to use the recursive text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap  = 10,\n",
        "    length_function = len,\n",
        ")"
      ],
      "metadata": {
        "id": "HSEw5nzXd9Mp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_and_chunk_pdfs():\n",
        "    loader = DirectoryLoader(f\"{root_dir}\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    return texts"
      ],
      "metadata": {
        "id": "1Ru60opfgzvM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to store embeddings\n",
        "def store_embeddings(docs, embeddings, store_name, path):\n",
        "    \"\"\"\n",
        "    Stores embeddings in FAISS format and saves to a pickle file.\n",
        "\n",
        "    Args:\n",
        "    - docs (list): List of documents.\n",
        "    - embeddings: Embedding model.\n",
        "    - store_name (str): Name of the embedding store.\n",
        "    - path (str): Path to the directory where embeddings will be stored.\n",
        "    \"\"\"\n",
        "    vector_store = FAISS.from_documents(docs, embeddings)\n",
        "    with open(os.path.join(path, f\"faiss_{store_name}.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(vector_store, f)"
      ],
      "metadata": {
        "id": "CjMDwKWNjFXZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load embeddings\n",
        "def load_embeddings(store_name, path):\n",
        "    \"\"\"\n",
        "    Loads embeddings from a pickle file.\n",
        "\n",
        "    Args:\n",
        "    - store_name (str): Name of the embedding store.\n",
        "    - path (str): Path to the directory where embeddings are stored.\n",
        "\n",
        "    Returns:\n",
        "    - vector_store: Loaded FAISS vector store.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(path, f\"faiss_{store_name}.pkl\"), \"rb\") as f:\n",
        "        vector_store = pickle.load(f)\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "pXk3ITs1jNIf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize Hugging Face Instruct Embeddings\n",
        "def initialize_huggingface_embeddings(model_name=\"hkunlp/instructor-xl\", device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Initializes Hugging Face Instruct Embeddings model.\n",
        "\n",
        "    Args:\n",
        "    - model_name (str): Name of the Hugging Face model.\n",
        "    - device (str): Device to run the model on.\n",
        "\n",
        "    Returns:\n",
        "    - embeddings: Initialized Hugging Face Instruct Embeddings model.\n",
        "    \"\"\"\n",
        "    return HuggingFaceInstructEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n"
      ],
      "metadata": {
        "id": "l3Z6wHCaiqWl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks(question, vector_store, num_chunks=1):\n",
        "    docs = vector_store.similarity_search(question, k=num_chunks)\n",
        "    return docs"
      ],
      "metadata": {
        "id": "DYMYa7FqxMGX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(question, chunks):\n",
        "    context = \"\\n\".join([chunk.page_content for chunk in chunks])\n",
        "    prompt = f\"Provide an answer to the following question using only the context provided: {question}? \" \\\n",
        "             f\"If you cannot answer this question from the information provided, respond with 'There is insufficient information to answer this question.'\\n\\n{context}\"\n",
        "    return prompt\n",
        "\n",
        "def gen_answer(prompt, max_length=100, temperature=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    response = model.generate(inputs, max_new_tokens=max_length, temperature=temperature)\n",
        "    answer = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "    return answer.strip()"
      ],
      "metadata": {
        "id": "Fw6BNukkw6w4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(question):\n",
        "\n",
        "  # initialize embedding model\n",
        "  instructor_embeddings = initialize_huggingface_embeddings()\n",
        "\n",
        "  # read and chunk data\n",
        "  texts = ingest_and_chunk_pdfs()\n",
        "\n",
        "  # create and store embeddings\n",
        "  store_embeddings(texts,\n",
        "                  instructor_embeddings,\n",
        "                  store_name='instructEmbeddings',\n",
        "                  path=embedding_store_path)\n",
        "\n",
        "  # Load the vector store\n",
        "  vector_store = load_embeddings(store_name='instructEmbeddings',\n",
        "                                      path=embedding_store_path)\n",
        "\n",
        "  # Load model and tokenizer with quantization\n",
        "  quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config, model_max_length=256)\n",
        "\n",
        "\n",
        "  # Retrieve relevant chunks based on the question\n",
        "  relevant_chunks = retrieve_relevant_chunks(question, vector_store)\n",
        "\n",
        "  # Format the prompt for the LLM\n",
        "  prompt = format_prompt(question, relevant_chunks)\n",
        "\n",
        "  # Generate the answer using the LLM\n",
        "  answer = gen_answer(prompt)\n",
        "  return answer\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o7z11h95w_nn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"What are the main causes of climate change?\"\n",
        "answer = main(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "8aZsfCmc4Obw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}